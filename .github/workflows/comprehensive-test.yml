name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM for nightly testing

jobs:
  unit-tests:
    name: Unit Tests - Python ${{ matrix.python-version }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12', '3.13']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
    
    - name: Install dependencies
      run: |
        uv venv
        source .venv/bin/activate
        uv pip install -e ".[dev]"
    
    - name: Run unit tests with coverage
      run: |
        uv run pytest tests/ -v --cov=cmdrdata --cov-report=xml --cov-report=term-missing
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false
    
    - name: Type checking with mypy
      run: |
        uv run mypy cmdrdata --strict
      continue-on-error: true
    
    - name: Lint with black and isort
      run: |
        uv run black --check cmdrdata tests
        uv run isort --check-only cmdrdata tests

  property-based-tests:
    name: Property-Based Testing (Hypothesis)
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
    
    - name: Install dependencies
      run: |
        uv venv
        source .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install hypothesis hypothesis-jsonschema
    
    - name: Run property-based tests
      run: |
        # Run hypothesis tests from the test suite
        uv run pytest tests/test_comprehensive.py::TestHypothesis -v --tb=short

  mutation-tests:
    name: Mutation Testing
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
    
    - name: Install dependencies
      run: |
        uv venv
        source .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install mutmut
    
    - name: Run mutation tests on critical modules
      run: |
        # Test critical billing and tracking code
        uv run mutmut run --paths-to-mutate cmdrdata/tracker.py --runner "pytest tests/"
      continue-on-error: true
    
    - name: Show mutation test results
      run: |
        uv run mutmut results
      if: always()

  integration-tests:
    name: Integration Tests (All Providers)
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
    
    - name: Install dependencies
      run: |
        uv venv
        source .venv/bin/activate
        uv pip install -e ".[dev]"
        uv pip install openai anthropic google-generativeai cohere
    
    - name: Run provider integration tests
      env:
        CMDRDATA_API_KEY: ${{ secrets.CMDRDATA_API_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
      run: |
        uv run python test_real_providers.py
      continue-on-error: true

  playwright-e2e:
    name: E2E Tests (Playwright)
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install uv
      uses: astral-sh/setup-uv@v3
    
    - name: Install dependencies
      run: |
        uv pip install pytest-playwright playwright
        uv run playwright install chromium
    
    - name: Create Playwright tests
      run: |
        cat << 'EOF' > test_e2e.py
        import pytest
        from playwright.sync_api import Page, expect
        
        BASE_URL = "https://api.cmdrdata.ai"
        
        def test_landing_page(page: Page):
            """Test landing page loads correctly"""
            page.goto(BASE_URL)
            expect(page).to_have_title("CmdrData - The Google Analytics of AI Usage")
            expect(page.locator("h1")).to_contain_text("Track Every Token")
        
        def test_demo_dashboard(page: Page):
            """Test demo dashboard is accessible"""
            page.goto(f"{BASE_URL}/demo/dashboard")
            expect(page).to_have_title("Dashboard - CmdrData")
            # Check for demo banner
            expect(page.locator(".demo-banner")).to_be_visible()
        
        def test_responsive_design(page: Page):
            """Test responsive design at different viewports"""
            viewports = [
                {"width": 375, "height": 667},   # Mobile
                {"width": 768, "height": 1024},  # Tablet
                {"width": 1920, "height": 1080}, # Desktop
            ]
            
            for viewport in viewports:
                page.set_viewport_size(viewport)
                page.goto(BASE_URL)
                
                # Navigation should be visible or have hamburger menu
                nav = page.locator("nav")
                expect(nav).to_be_visible()
                
                # Hero section should be readable
                hero = page.locator(".hero-section")
                expect(hero).to_be_visible()
        
        def test_documentation_links(page: Page):
            """Test that documentation links work"""
            page.goto(BASE_URL)
            
            # Check SDK documentation link
            sdk_link = page.locator('a:has-text("Documentation")')
            if sdk_link.count() > 0:
                sdk_link.first.click()
                expect(page).to_have_url(/docs|documentation/)
        
        def test_api_health_endpoint(page: Page):
            """Test API health endpoint"""
            response = page.request.get(f"{BASE_URL}/health")
            assert response.status == 200
            data = response.json()
            assert data["status"] == "healthy"
        
        if __name__ == "__main__":
            pytest.main([__file__, "-v", "--browser", "chromium", "--headed"])
        EOF
    
    - name: Run E2E tests
      run: |
        uv run pytest test_e2e.py -v --browser chromium
      continue-on-error: true
    
    - name: Upload test artifacts
      uses: actions/upload-artifact@v4
      if: failure()
      with:
        name: playwright-traces
        path: test-results/

  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Bandit security scan
      run: |
        pip install bandit
        bandit -r cmdrdata/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Check for hardcoded secrets
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD

  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install pytest-benchmark cmdrdata
    
    - name: Run performance benchmarks
      run: |
        cat << 'EOF' > test_performance.py
        import pytest
        from cmdrdata import CmdrData
        
        def test_wrapper_overhead(benchmark):
            """Benchmark the overhead of wrapping a client"""
            def create_wrapper():
                return CmdrData(
                    client=None,
                    cmdrdata_api_key="test-key",
                    customer_id="test-customer"
                )
            
            result = benchmark(create_wrapper)
            assert result is not None
        
        def test_metadata_processing(benchmark):
            """Benchmark metadata processing"""
            metadata = {"key": "value", "nested": {"data": "test"}} * 100
            
            def process_metadata():
                client = CmdrData(
                    client=None,
                    cmdrdata_api_key="test-key",
                    metadata=metadata
                )
                return client.metadata
            
            result = benchmark(process_metadata)
            assert result is not None
        EOF
        
        pytest test_performance.py -v --benchmark-only
      continue-on-error: true

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, property-based-tests, mutation-tests, integration-tests, playwright-e2e, security-scan, performance-benchmarks]
    if: always()
    
    steps:
    - name: Test Results Summary
      run: |
        echo "## Test Suite Summary"
        echo "Unit Tests: ${{ needs.unit-tests.result }}"
        echo "Property-Based Tests: ${{ needs.property-based-tests.result }}"
        echo "Mutation Tests: ${{ needs.mutation-tests.result }}"
        echo "Integration Tests: ${{ needs.integration-tests.result }}"
        echo "E2E Tests: ${{ needs.playwright-e2e.result }}"
        echo "Security Scan: ${{ needs.security-scan.result }}"
        echo "Performance: ${{ needs.performance-benchmarks.result }}"